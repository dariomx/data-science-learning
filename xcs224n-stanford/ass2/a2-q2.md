---
output:
  pdf_document: default
  html_document: default
---
Prove that

\[
\frac{\partial J}{\partial v_c} = U(\hat{y} - y)
\]

Based on this equation:

\[
J_{naive-softmax}(U_c, o, U) = -log(\hat{y_o})
\]

Proof:

We accept the initial help and reproduce the steps here: 


\begin{align*}
J_{naive-softmax}(U_c, o, U) &= -log(\hat{y_o}) \\\\
&= -log\left( \dfrac{exp(u_o^T v_c)}{\sum_{w \in Vocab} exp(u_w^T v_c)} \right) && \text{by definition of $\hat{y}_o$} \\\\
&= -\left( log(exp(u_o^T v_c)) - log\left(\sum_{w \in Vocab} exp(u_w^T v_c)\right) \right) && \text{by laws of logarithms} \\\\
&= -u_o^T v_c + log\left(\sum_{w \in Vocab} exp(u_w^T v_c)\right) && \text{exp  is the inverse of log, and sign laws}
\end{align*}


Let us continue the proof, by taking the derivative respecto t $v_c$: 


\begin{align*}
\frac{\partial J}{\partial v_c} &= \frac{\partial}{\partial v_c} \left( -u_o^T v_c + log\left(\sum_{w \in Vocab} exp(u_w^T v_c)\right)\right) && \text{taking derivative on both sides} \\\\
&= \frac{\partial}{\partial v_c} \left( -u_o^T v_c \right) +  \frac{\partial}{\partial v_c} \left( log\left(\sum_{w \in Vocab} exp(u_w^T v_c)\right)\right) && \text{derivative of a sum} \\\\
&= -u_o +  \frac{\partial}{\partial v_c} \left( log\left(\sum_{w \in Vocab} exp(u_w^T v_c)\right)\right) && \frac{\partial}{\partial v_c} \left( -u_o^T v_c \right) = -u_o \\\\
&= -u_o +  \dfrac{\frac{\partial}{\partial v_c} \left( \sum_{w \in Vocab} exp(u_w^T v_c)\right)}{\sum_{w \in Vocab} exp(u_w^T v_c)} && \text{derivative of log} \\\\
&= -u_o +  \dfrac{\sum_{w \in Vocab} \frac{\partial}{\partial v_c} \left(exp(u_w^T v_c)\right)}{\sum_{w \in Vocab} exp(u_w^T v_c)} && \text{derivative of a sum} \\\\
&= -u_o +  \sum_{w \in Vocab} \left(\dfrac{\frac{\partial}{\partial v_c} \left(exp(u_w^T v_c)\right)}{\sum_{w' \in Vocab} exp(u_{w'}^T v_c)}\right) && \text{re-arranging the sum and renaming index variable} \\\\
&= -u_o +  \sum_{w \in Vocab} \left(\dfrac{exp(u_w^T v_c) \frac{\partial}{\partial v_c} \left( u_w^T v_c \right) }{\sum_{w' \in Vocab} exp(u_{w'}^T v_c)}\right) && \text{derivative of exp} \\\\
&= -u_o +  \sum_{w \in Vocab} \left(\dfrac{exp(u_w^T v_c) u_w }{\sum_{w' \in Vocab} exp(u_{w'}^T v_c)}\right) && \frac{\partial}{\partial v_c} \left( u_w^T v_c \right) = u_w \\\\
&= -u_o +  \sum_{w=1}^V \left(\dfrac{exp(u_w^T v_c)}{\sum_{w' \in Vocab} exp(u_{w'}^T v_c)}\right) u_w && \text{associativity law and changing index notation} \\\\
&= \sum_{w=1}^V \hat{y}_w u_w - u_o&& \text{definition of $\hat{y}_w$ and commutativity law} \\\\
&= U \hat{y} - U y && \text{$Ax = \sum x_i a_i$, and definition of $\hat{y}$ and $y$} \\\\
&= U (\hat{y} - y) && \text{distributivity law $\square$} \\
\end{align*}

Note: The previous to last step perhaps requires more explanation, as we are using the fact that the product of a matrix times a vector, can be seen as a linear combination of the columns of the matrix (the coefficients would be the elements of the vector). Also, per definition, $y$ would have all zeros except for the index $o$ which would be $1$. Hence, in developing the linear combination of the columns of $U$, only $u_o$ will survive. 