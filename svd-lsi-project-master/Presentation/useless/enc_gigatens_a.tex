\begin{frame}[plain]
	\frametitle{Now we are talking! Gigatensor [Kang12]}
	\begin{block}{}
    Many data are modeled as tensors, or multi dimensional arrays. 
    ... Tensor decomposition is an important data
    mining tool with various applications ... However, current tensor
    decomposition algorithms are 
    not scalable for large tensors with billions of sizes and hundreds
    millions of nonzeros: the largest tensor in the literature remains
    thousands of sizes and hundreds thousands of nonzeros. 
	\end{block}
	\begin{block}{}
    Consider a knowledge base tensor consisting of about 26 million
    noun-phrases. The intermediate data explosion problem, associated
    with naive implementations of tensor decomposition algorithms,
    would require the materialization and the storage of a ma-  
    trix whose largest dimension would be $\sim$ 7 Â· 1014 ; this amounts to 
    $\sim$ 10 Petabytes, or equivalently a few data centers worth of storage, 
    thereby rendering the tensor analysis of this knowledge base, in the 
    naive way, practically impossible. 
	\end{block}
\end{frame}
