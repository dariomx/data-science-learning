\section{Merging Two SVD factorizations}

The core logic of the algorithms presented in last section
(\cref{alg:svd-dist} and \cref{alg:svd-dist-node}), relies on the
procedure \func{Merge-SVD}. Is may not be evident at all, but the
essence of this merge is to use SVD factorization again! The The PhD
thesis of \Rehurek presents a series of refinements, until he reaches
the optimized version presented below: \\

\begin{algorithm}
  \label{alg:merge-svd}
  \caption{$\func{Merge-SVD}$: Merge of two SVD factorizations}
%
  \setstretch{1.35}
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
%
  \Input{Truncation factor $k$, decay factor $\gamma$,  
    $P_1 = (U_1^{m \times k_1}, \Sigma_1^{k_1 \times k_1})$,
    $P_2 = (U_2^{m \times k_2}, \Sigma_1^{k_2 \times k_2})$}
%
  \Output{$(U^{m \times k}, \Sigma^{k \times k})$}
%
  $Z^{k_1 \times k_2} \gets \trans{U_1}U_2$ \;
%
  $\prim{U} R \xleftarrow{QR} U_2 - U_1 Z$ \;
%
  $U_R \Sigma\trans{V_R} \xleftarrow{SVD_k}
    \begin{bmatrix}
      \gamma\Sigma_1 & Z \Sigma_2 \\
      0 & R\Sigma_2
    \end{bmatrix}^{(k_1 + k_2) \times (k_1 + k_2)}$ \;
%
  $\begin{bmatrix}
      R_1^{k_1 \times k} \\
      R_2^{k2 \times k}
    \end{bmatrix} = U_R$ \;
%
  $U \gets U_1R_1 + \prim{U}R_2$ \;
%
  return $(U,\Sigma)$ \;
\end{algorithm}
\hfill

The \cref{alg:merge-svd} is a quite compressed piece of work, and none
of its steps are intuitive. We proceed to explain them in more detail
in the following subsections.

\subsection{Input and Output Parameters}

Is worth to remark a couple of new features that appear as input
parameters of the merge procedure: we are introducing a new decay
factor $\gamma \in (0.0,1.0)$ that helps to give less relevance to old
documents. Let us recall that these algorithms are designed to update
an existing SVD calculation, where each update processes a new set
documents (encoded as columns of the term-document matrix $A$). \\

There are three truncation parameters ($k$, $k_1$ and $k_2$), instead of
just one; this is to give further flexibility to the algorithm, as it
supports that the truncation factor varies with time. Each of the
previous factorizations then could have been done with different
truncation factors; but we homogenize the final result with the new
truncation factor $k$. This feature may not be heavily used, as
usually $k$ is fixed in a few hundreds and not changed during the
entire life of the LSI applications; there is no need though, to loose
generality and impose the artificial restriction that the truncation
factor shall remain static. \\

The output parameter, or result of the merge algorithm, is a new
factorization $U,\Sigma$ which covers the two partial SVD
factorizations received. 

\subsection{Construction of a new basis}

Most of the algorithm is about building a new basis (columns of matrix
$U$), that spans the subspaces generated by basis in $U_1$ and $U_2$,
respectively. This is done by taking advantage that $U_1$ and $U_2$
have orthonormal basis already as columns; hence, one of them is
picked ($U_1$), and we only build the delta $\prim{U}$ required to
extend  basis $U_1$ into required basis $U$. \\

The first two lines of \cref{alg:merge-svd} are basically to build the
delta basis $\prim{U}$, and thought not evident (nor explained in the
articles by \Rehurek), we can find an intuitive interpretation of this
procedure. Let us think in two vectors in \R{3} named 
\vec{u_1} and \vec{u_2}, which are linearly
independent. Let us suppose that we are given the task of building a
basis of the two-dimensional subspace that those vectors span, with
the additional requirement of making such basis orthonormal. Let us
suppose that we pick \vec{u_1} to be part of the basis, and now we
just need to find an orthogonal vector to $u_1$, in order to complete our
task. It can be proven that if we subtract from 
\vec{u_2} the projection of \vec{u_2} into \vec{u_1}, we get a vector
that is orthogonal to \vec{u_1} (let us name it $\vec{u_3}$):

\[
\vec{u_3} = \vec{u_1} - (\vec{u_2} \cdot \vec{u_1}) \vec{u_1} \suchthat
\vec{v_3} \ds{\perp} \vec{u_1}
\]
\hfill

If we consider the resulting set from the above recipe, that is $\{\vec{u_1},
\vec{u_3}\}$, we can not tell yet that is an orthonormal
basis. However, they are at least linearly independent, hence we can
apply standard procedures like Gram-Schmidt (see \cite{strang88}) to
produce the desired orthonormal basis. \\


Of course the above recipe works for any dimension, and that is
essentially the calculation done in the first two lines of
\cref{alg:merge-svd}; though it 
states all the vector equations at once, by using matrix
notation (the columns of matrices $U_1$ and $U_2$ play the role of
vectors \vec{u_1} and \vec{u_2} from our example; and the right side
of assignment of line two corresponds to vector \vec{u_3}). On the
first line we calculate matrix $Z$ which is the projection matrix of
the columns of $U_2$ into columns from $U_1$; this give us the
component of the projections only (the dot products), but multiplying
that by $U_1$ is equivalent to the expression $(\vec{u_2} \cdot
\vec{u_1}) \vec{u_1}$ from our example. The matrix
substraction is a compressed way of introducing the vector equations
from our example; and the QR factorization used to produce the
orthonormal basis, is basically the application of the Gram-Schmidt
process that we mentioned as well. It is not mentioned by
\cite{rehurek11a} but the way of calculating 
$\prim{U}$ is quite similar (if not the same), to the one reported by
Hall et all in \cite{hall00} and \cite{hall02} (where it is done in
the context of merging eigen models, which in particular contain eigen
decompositions). \\

The usage of factorization $QR$ deserves more comments, as we have not
mentioned much about it until now. Given a rectangular matrix $B^{m
  \times n}$, it produces a factorization which consists of an
  orthogonal matrix $Q^{m \times m}$ (which is essentially a basis
  for the subspace spanned by the columns of $A$); followed by an
  upper triangular matrix $R^{m \times 
    n}$. The triangular form of $R$ 
  comes from the application of the Gram-Schmidt algorithm: the column
  $R_1$ contains the coordinates of original column $A_1$ respect to
  the basis $Q$ (it only depends on $Q_1$), the column $R_2$ indicates
  that original column $A_2$ depends only on the first two columns of
  $Q$, and so on. The QR algorithm is chosen by \Rehurek, not only due
  its ability to produce the missing vectors we needed for our basis
  (matrix $\prim{U}$); but also due its side product, the triangular matrix
  $R$ which is used in further steps.

\subsection{Producing the diagonal matrix $\Sigma$}

The probably most obscure step appears in line $3$, where another SVD
factorization is being applied, in order to produce the first part of
the final result (the diagonal matrix $\Sigma$); along with an
auxiliary rotation that we need to 
produce the other half of the final result (matrix $U$). But let us connect
this with previously used QR algorithm (line $2$), in order to clarify
further. \\

In the SVD literature, there is a variant called R-SVD which uses the
QR factorization as an intermediate step for SVD calculation. The
name seems to come from Golub's book \cite{golub13}, where is
introduced as a previous step to the so called R-Bidiagonalization
(the method proposed by Golub brings the original matrix $A$ to a
bidiagonal form, from where calculating SVD is easier). Putting aside
this bidiagonalization context, the main idea of using QR
factorization as an intermediate step in SVD calculation, is
summarized in equation below: \\

\begin{equation}
\label{eq:svd-qr}
A = QR = Q (\prim{U} \Sigma \trans{V}) = (Q \prim{U}) \Sigma \trans{V}
\end{equation}
\hfill

We can appreciate from equation above that the final matrix $U$ is
obtained, by composing the $\prim{U}$ matrix (from the SVD factorization
of triangular matrix $R$), with the orthogonal matrix  $Q$ (obtained from the
QR factorization of $A$). Interestingly, the matrices $\Sigma$ and $V$ from
the SVD of $R$, become the same as if one would have done SVD directly
on matrix $A$. This is essentially the idea of line $3$ from
\cref{alg:merge-svd}, which produces the diagonal matrix $\Sigma$ that
we need as final result; but it also produces a couple of additional matrices:

\begin{itemize}
  \item The orthogonal matrix $V_R^{T}$, which is discarded (let us
    recall we just care about $U$ and $\Sigma$). 
  \item The matrix $U_R$, which like in the example with $QR$
    factorization, is just an auxiliary item for producing the final
    matrix $U$ that we need (more about this on next section).
\end{itemize}
\hfill

But the side products of the ${SVD}_k$ calculation on step $3$ is
perhaps the less problematic to understand, the real trouble may come
from the matrix we are using as input for such calculation. Let us
name such matrix on the right hand side as $X$, it can be deduced from
the following requirement that we impose on the final matrix $U
= \begin{bmatrix}U_1 \mid \prim{U}\end{bmatrix}$ \footnote{The
  equality claimed on this 
  equation is not totally clear, as after the $SVD_k$ calculation of
  $X$ we drop its $V$ matrix; and the left side does not involve any
  matrix $V$. We contacted a couple of times the author (Radim
  \Rehurek) for \href{http://math.stackexchange.com/questions/1375029/merging-two-svd-factorizations-but-using-only-u-1s-1-and-u-2s-2-question-ab}{kindly
  asking for a clarification about a related equation in his thesis},
  but unfortunately we did not got a final answer.}: \\

\[
\begin{bmatrix}U_1\Sigma_1 \mid U_2\Sigma_2\end{bmatrix} = 
\begin{bmatrix}U_1 \mid \prim{U}\end{bmatrix} X
\]
\hfill

If we clear the matrix variable $X$ by multiplying each side (on the
left) by \trans{\begin{bmatrix}U_1 \mid \prim{U}\end{bmatrix}}, we get
the following (please note that we are using the matrix block
operations, which nicely behave like scalars):

\begin{equation}
\label{eq:svd-merge-x1a}
X = 
\trans{\begin{bmatrix}U_1 \mid \prim{U}\end{bmatrix}} \begin{bmatrix}U_1\Sigma_1 \mid U_2\Sigma_2\end{bmatrix} =
\begin{bmatrix}
\trans{U_1}U_1\Sigma_1 & \trans{U_1}U_2\Sigma_2 \\
\trans{\prim{U}}U_1\Sigma_1 & \trans{\prim{U}}U_2\Sigma_2
\end{bmatrix}
\end{equation}
\hfill

We need now a few additional equalities that can be inferred from the
\cref{alg:merge-svd}: \\

\begin{enumerate}
\item $U_1$ is orthogonal $\implies$ $\trans{U_1}U_1 = I$ \\
\item By construction, the set of columns from where $\prim{U}$ is
  calculated (that is, $U_2 - U_1Z$), is orthogonal to $U_1$ $\implies$
  the subspace spanned by such set is also orthogonal to $U_1$. In
  particular, any basis of that subspace is also orthogonal to
  $U_1$. Therefore $\prim{U}$ is orthogonal to $U_1$, that is,
  $\trans{\prim{U}} U_1 = 0$. \\ 
\item Using the above, and the QR calculation from line 2 of
  \cref{alg:merge-svd}, \Rehurek claims that $R =
  \trans{\prim{U}} U_2$. Such equality is not totally clear, as it seems
  as if we would be isolating $R$ from that step; however, such step
  represents an assignment, not an equation. The claim may be due may
  a property of QR calculation itself (seen as a function of matrices, rather
  than a procedure). We take it for granted \footnote{If this work is
    used for a thesis, we will seek to clarify this part though.}.
\end{enumerate}
\hfill

Using the three equalities just mentioned, the matrix $X$ from
\cref{eq:svd-merge-x1a} can be further simplified as follows: \\

\begin{equation}
\label{eq:svd-merge-x1b}
X = 
\begin{bmatrix}
\Sigma_1 & \trans{U_1}U_2\Sigma_2 \\
0        & \trans{\prim{U}}U_2 \Sigma_2
\end{bmatrix} =
\begin{bmatrix}
\Sigma_1 & Z\Sigma_2 \\
0        & R\Sigma_2
\end{bmatrix} 
\end{equation}
\hfill

It is equation \cref{eq:svd-merge-x1b} that justifies the right hand
side of step 3 in \cref{alg:merge-svd}. \\

A final note about this step, is that the ${SVD}$ routine being called
is not the same as $\func{Basecase-SVD}$ from \cref{alg:svd-dist};
while the former is a full SVD for shorter ``dense'' matrices, the
second is a truncated SVD calculation for large sparse 
ones. The dense SVD calculation is done with the standard algorithm
called Golub-Kahan-Reinsch (\cite{golub75}, \cite{golub70}), available
as a LAPACK routine \cite{lapack}); while the
truncated sparse SVD is done with the also fameous LASVD routine (later
incarnated as SVDPACKC LAS2), that Berry did from the Lanzos version
of Parlett and Simon (\cite{parlett79},\cite{simon84}). 

\subsection{Calculating the final matrix $U$}

All these auxiliary results may take us apart from our final goal, so
let us remember what it is: to produce a couple of matrices, $U$ and
$\Sigma$, which represent the merged eigen decomposition of the two
pair of matrices we received as input ($U_1$,$\Sigma_1$ and
$U_2$,$\Sigma_2$). So far, we have calculated already the diagonal
$\Sigma$; hence the remaining task is to calculate $U$. We have all
the auxiliary devices at our disposal, from previous steps of the
algorithm. \\

We began by picking orthonormal basis $U_1$, and extending it with
$\prim{U}$ in order to get a new orthonormal basis (in matrix form)
$\begin{bmatrix}U_1 \mid \prim{U}\end{bmatrix}$; such basis covers the
spanning subspaces of both $U_1$ and $U_2$. We may be tempted to
think that such matrix is the desired $U$, but the problem is that we
took the diagonal $\Sigma$ from an $SVD$ calculation; that means we
got already one orthogonal matrix for the term-space $U_R$. We need to
compose such $U_R$ with our orthonormal basis 
$\begin{bmatrix}U_1 \mid  \prim{U}\end{bmatrix}$, in order to get the final basis $U$
(due same reasons exposed in \cref{eq:svd-qr}): \\

\[
U = \begin{bmatrix}U_1 \mid  \prim{U}\end{bmatrix} U_R
\]
\hfill

But now we exploit the shape of matrix $U_R$; if we were doing full SVD
calculation in the line $3$ of \cref{alg:merge-svd}, we would have a
matrix $U_R$ of dimensions $(k_1+k_2) \times (k_1+k_2)$. But since we
are calculating the truncated SVD instead, it gets dimensions
$(k_1+k_2) \times k$. Furthermore, it can be split in two blocks $R_1$
and $R_2$ as follows: 

\[
U_R = 
\begin{bmatrix}
R_1^{k_1 \times k} \\[0.4em]
R_2^{k_2 \times k}
\end{bmatrix}
\]
\hfill

Using block multiplication in the submatrices, we can get the final
assignment from line $5$ of \cref{alg:merge-svd}: \\

\[
U \leftarrow 
\begin{bmatrix}U_1 \mid  \prim{U}\end{bmatrix} U_R =
\begin{bmatrix}U_1 \mid  \prim{U}\end{bmatrix} 
\begin{bmatrix}
R_1^{k_1 \times k} \\[0.4em]
R_2^{k_2 \times k}
\end{bmatrix} =
U_1R_1 + \prim{U}R_2
\]
\hfill
