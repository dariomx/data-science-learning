\begin{frame}[plain]
	\frametitle{Enter matrix/tensor decomposition [Turney07]}
	\begin{block}{}
    In IR, SVD is typically applied to a term $\times$
    document matrix. It smoothes the weights, so that a document $d$
    will have a nonzero weight for a word $w$ if $d$ is similar to other
    documents that contain the word $w$, even if $d$ does not contain
    actually contain $w$. 
	\end{block}
	\begin{block}{}
    To extend the term-document matrix to a third-
    order tensor, it would be natural to add information
    such as author, date of publication, citations, and
    venue, etc.
	\end{block}
	\begin{block}{}
    In our recent work, we have begun exploring ten-
    sor decompositions for semantic space models. We
    are currently developing a word $\times$ pattern $\times$ word
    tensor that can used for both synonyms and analo-
    gies (multiple-choice TOEFL questions).
	\end{block}
\end{frame}
