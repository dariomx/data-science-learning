\begin{frame}[plain]
	\frametitle{What was big for [Curran02]?}
	\begin{block}{}
    We combined the BNC and Reuters corpus to
    produce a 300 million word corpus. The sentences
    were randomly shuffled together to produce a single homogeneous
    corpus. This corpus was split into 
    two 150M word corpora over which the main experimental results are
    averaged. We then created smaller 1/2 down to 1/64th of each 150M
    corpus. The next section describes the method of evaluating each
    thesaurus created by the combination of a given context extraction  
    system and corpus size.
	\end{block} 
	\begin{block}{}
    For each term we extracted a thesaurus entry with 200 
    potential synonyms and their weighted Jaccard scores.
	\end{block} 
	\begin{block}{}
    Improvement gained by morphological analysis of the attributes: 
    345mb $\rightarrow$ 302mb $\rightarrow$ 272mb.
	\end{block} 
\end{frame}
