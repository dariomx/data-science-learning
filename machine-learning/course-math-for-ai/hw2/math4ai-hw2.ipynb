{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "<b>If the square matrix A is not invertible, why is it \"likely\" that the inho-\n",
    "mogeneous equation is inconsistent? \"Likely\", in this case, means that the\n",
    "system should be inconsistent for a y chosen at random</b>\n",
    "\n",
    "If it is not invertible, it means that when we take it to its reduced form, at least one of the rows will not have a leading entry; that is, will be all zeros. Now, if we imagine that we put a vector $\\vec{y}$ on the right side with $A\\vec{x} = \\vec{y}$, then zooming into that particular row which became zero will give us this equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "0x_1 + 0x_2 \\cdots + 0x_m = y_i\n",
    "\\end{equation*}\n",
    "\n",
    "Now, in order to have solutions on the system we need a $y_i$ that is non zero. But all its possible values are $\\mathbb{R}$, thus, is certainly more likely that we pick a non zero entry. Therefore, if we pick a $y \\in \\mathbb{R}$ randomly, is more likely that the system $A\\vec{x} = \\vec{y}$ will not have solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "<b>If A and B are square matrices of the same size, then $det(AB) = det(A)det(B)$ \n",
    "\n",
    "(a) Prove this last statement (Hint the elementary matrices).</b>\n",
    "\n",
    "We will have two sub-proofs, for the respective cases of $A$ being invertible or not (same could be done for $B$, but let us just choose $A$).\n",
    "\n",
    "#### case 1) $A$ is not invertible\n",
    "\n",
    "There is another result that claims that if $A$ is singular and square, and if multiplied by another arbitrary matrix $B$; then the result $AB$ will be another singular matrix. In other words, this results tells us that the singularity is like a disease that gets transmited through multiplication. \n",
    "\n",
    "Is not hard to prove if we think in terms of linears transformations. The following chain of logical statements does the informal proof:\n",
    "\n",
    "<ul>\n",
    "<li>Assume $B$ is a biyection of $\\mathbb{R}^n \\implies \\mathbb{R}^n$.</li>\n",
    "<li>Assume $A$ is singular.</li>\n",
    "<li>$A$ sends a non-trivial subspace $N(A)$, the kernel, into zero.</li>\n",
    "<li>$AB$ is a composition of two linear transformations $B$ and $A$.</li>\n",
    "<li>Since $A$ is not a biyection, once we compose $B$ with $A$ through $AB$, we are doomed to loose biyection of $B$ ($AB$ will have non trivial kernel too). </li>\n",
    "<li>Therefore, $AB$ is non singular as well.</li>\n",
    "</ul>\n",
    "\n",
    "Now, let us remember another result (the negated form of this is proved in exercise 4).\n",
    "\n",
    "$A$ is singular $\\iff$ $det(A)=0$\n",
    "\n",
    "Using these two weapons, we can know prove this subcase of the theorem, because we know that both $A$ and $AB$ are singular:\n",
    "\n",
    "$\\therefore\\; det(AB) = 0 = 0\\, det(B) = det(A)\\, det(B)$\n",
    "\n",
    "#### case 2) $A$ is invertible\n",
    "\n",
    "Let us know move into the more interesting case of $A$ being invertible. \n",
    "\n",
    "If $A$ is invertible then $\\exists\\,A^{-1}$ and $A^{-1}\\,A = I$. We also know that the inverse is nothing but the application of elementary matrices (those that took $A$ into $I$ in Guass-Jordan algorithm):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\prod_{i=1}^k E_i = A^{-1}\n",
    "\\end{equation*}\n",
    "\n",
    "But by properties of elementary matrices and the product, we know that $A$ is also built out of elementary matrices (precisely, the inverse of those $E_i$):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\prod_{i=1}^k E_{k-i+1}^{-1} = A\n",
    "\\end{equation*}\n",
    "\n",
    "Let us use above theorem and rewrite a little $det(AB)$\n",
    "\n",
    "\\begin{equation*}\n",
    "det(AB) = det\\left(\\prod_{i=1}^k E_{k-i+1}^{-1} B\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "On the other hand, in notes and in class we saw the that the elementary matrices do not alter much the determinant, actually, they just vary original determinant by a constant (last statement refers to the operation of adding a multiple of another row):\n",
    "\n",
    "<ol>\n",
    "<li>$det(cA) = cdet(A)$</li>\n",
    "<li>$det(swap(A,i,j)) = -det(A)$</li>\n",
    "<li>$det(\\{A_i := A_i + kA_j\\}) = det(A)$</li>\n",
    "</ol>\n",
    "\n",
    "Putting together these results, we can prove a kind of useful theorem that says something like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "det\\left(\\prod_{i=1}^k E_i A\\right) = \\left(\\prod_{i=1}^k \\alpha_i\\right)\\,det(A) = \\alpha \\, det(A)\n",
    "\\end{equation*}\n",
    "\n",
    "in other words, we are saying that applying a series of elementary matrices does not alter the original determinant beyond a constant. This could be proved by example, using induction over the $n$ and doing analysis by cases depending on the type of elementary operation; but we will omit this proof for brevity.\n",
    "\n",
    "Applying the latest theorem to our development of $A$ as product of elementary matrices, leads us to:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "det(AB) = det\\left(\\prod_{i=1}^k E_{n-i+1}^{-1} B\\right) = \\alpha\\, det(B)\n",
    "\\end{equation*}\n",
    "\n",
    "We could take out the $det(B)$, and what is left is to prove that $\\alpha = det(A)$. For this we express again $A$ as the product of elementary matrices (same ones used above), and the fact that $A\\,A^{-1} = I$\n",
    "\n",
    "\\begin{equation*}\n",
    "1 = det(I) = det(A\\,A^{-1}) = det\\left(\\prod_{i=1}^k E_{n-i+1}^{-1} A^{-1}\\right) = \\alpha\\,det(A^{-1})\n",
    "\\end{equation*}\n",
    "\n",
    "Above development immediately leads to expected result (using an additional theorem that relates the determinants of $A$ and $A^{-1}$):\n",
    "\n",
    "\\begin{equation*}\n",
    "1 = \\alpha\\,det(A^{-1}) \\;\\implies\\; \\alpha = \\frac{1}{det(A^{-1})} = \\frac{\\frac{1}{1}}{\\frac{1}{det(A)}} = det(A)\n",
    "\\end{equation*}\n",
    "\n",
    "We can conclude now this case:\n",
    "\n",
    "\\begin{equation*}\n",
    "det(AB) = \\alpha\\,det(B) \\,\\land\\, det(A) = \\alpha \n",
    "\\;\\implies\\; det(AB) = det(A)\\,det(B)\\; \\therefore\n",
    "\\end{equation*}\n",
    "\n",
    "The two sub-proofs, for $A$ being singular and invertible, compose the proof of the whole theorem and we are done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "<b>If A is an upper triangular matrix with one or more 0's on the main\n",
    "diagonal, then det(A) = 0.</b>\n",
    "\n",
    "In class we saw that if $A$ is upper triangular, then\n",
    "\n",
    "\\begin{equation*}\n",
    "det(A) = \\prod_{i=1}^k A_{ii}\n",
    "\\end{equation*}\n",
    "\n",
    "clearly, if any of those $A_{ii}$ becomes zero the whole product becomes zero; leading to $det(A) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "<b>Show that A is invertible $\\iff$ $det(A) \\neq 0$.</b>\n",
    "\n",
    "#### first part, $A$ invertible $\\implies$ $det(A) \\neq 0$\n",
    "\n",
    "If $A$ is invertible then we can apply elementary operations against it, and take it to upper triangular form $R$ (echelon form). But per previous results mentioned in this homework, we know that doing that will vary the original determinant by a constant only.\n",
    "\n",
    "\\begin{equation*}\n",
    "det(R) = det\\left(\\prod_{i=1}^k E_i\\,A\\right) = \\gamma\\,det(A)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, since $A$ is invertible we know that $R$ will have all its diagonal with leading entries (ones). We also know how to compute the determinant of $R$\n",
    "\n",
    "\\begin{equation*}\n",
    "det(R) = \\prod_{i=1}^n R_{ii} = 1\n",
    "\\end{equation*}\n",
    "\n",
    "Combining these two results leads to:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\gamma\\,det(A) = 1 \\;\\implies\\; det(A) = \\frac{1}{\\gamma}\n",
    "\\end{equation*}\n",
    "\n",
    "$det(A)$ must be non zero, because $\\frac{1}{\\gamma}$ is well defined and non zero. This is because $\\gamma$ comes from the application of elementary matrices (the constants they add to $det(A)$ are never zero). \n",
    "\n",
    "#### second part, $det(A) \\neq 0$ $\\implies$ $A$ invertible\n",
    "\n",
    "Let us apply again Gauss-Jordan against $A$ and obtain its echelon form $R$. We know that the determinant of this matrix relates to $det(A)$ by a constant.\n",
    "\n",
    "\\begin{equation*}\n",
    "det(R) = \\beta \\, det(A)\n",
    "\\end{equation*}\n",
    "\n",
    "If $det(A) \\neq 0$ then $det(R) \\neq 0$ (by above equation, as $\\beta$ is non zero for coming out of elementary operations). \n",
    "\n",
    "Now, if $R$ has a non zero determinant, it means that all its diagonal has non zero elements. But this will mean that all its columns are linearly independent, which in turn implies the same for $A$. \n",
    "\n",
    "And finally, by another theorem seen in the notes, we know that if $A$ has all its columns linearly independent then $A$ is invertible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "<b>Implement the calculus of the determinant using the echelon idea.</b>\n",
    "\n",
    "We already have implemented the Gauss-Jordan algorithm, by left multiplying original matrix $A$ by elementary matrices $E_i$. By the theorems referred in previous exercises, we know that each one of those $E_i$ will alter the $det(A)$ by a constant only. Let us supposed that $A$ is invertible, hence the determinant of its reduced echelon form will be $1$; leading to:\n",
    "\n",
    "\\begin{equation*}\n",
    "1 = det(R) = det\\left(\\prod_{i=1}^k E_i \\,A\\right) = \\gamma\\,det(A)\n",
    "\\end{equation*}\n",
    "\n",
    "In other words:\n",
    "\n",
    "\\begin{equation*}\n",
    "det(A) = \\frac{1}{\\gamma}\n",
    "\\end{equation*}\n",
    "\n",
    "That is, we just multiply the constants associated with the $E_i$; and if the result is non zero, we return its inverse multiplicative as $det(A)$. If the product is zero, we know that $det(A) = 0$.\n",
    "\n",
    "Modified code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BRO_MULT = 1\n",
    "BRO_ADDMULT = 2\n",
    "BRO_SWAP = 3\n",
    "\n",
    "class BasicRowOper:\n",
    "    def __init__(self, bro_type, src, k, dst):\n",
    "        self.bro_type = bro_type\n",
    "        self.src = src\n",
    "        self.k = k\n",
    "        self.dst = dst\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"(%s, %d, %f, %d, det_cte=%f)\" % (self.bro_type, \\\n",
    "                                     self.src, self.k, self.dst, \\\n",
    "                                     self.det_cte)\n",
    "        \n",
    "    # builds the matrix that represents the basic operation\n",
    "    def _basic_matrix(self, n):\n",
    "        bmat = np.identity(n)\n",
    "        if self.bro_type == BRO_MULT:      \n",
    "            assert self.dst == self.src\n",
    "            bmat[self.dst] = self.k * bmat[self.src]\n",
    "        elif self.bro_type == BRO_ADDMULT:\n",
    "            bmat[self.dst] += self.k * bmat[self.src]\n",
    "        elif self.bro_type == BRO_SWAP:\n",
    "            bmat[[self.src, self.dst]] = bmat[[self.dst, self.src]]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid basic row operation type\")\n",
    "        return bmat\n",
    "    \n",
    "    # also returns the cte associated with elementary matrix \n",
    "    def apply(self, mat):\n",
    "        n, _ = mat.shape\n",
    "        mat[:,:] = np.dot(self._basic_matrix(n), mat)         \n",
    "        print(\"%s: after\\n%s\\n\" % (self, mat))\n",
    "        return self.det_cte\n",
    "    \n",
    "class BroMult(BasicRowOper):\n",
    "    def __init__(self, k, dst):\n",
    "        BasicRowOper.__init__(self, BRO_MULT, dst, k, dst)\n",
    "        self.det_cte = k\n",
    "\n",
    "class BroAddMult(BasicRowOper):\n",
    "    def __init__(self, src, k, dst):\n",
    "        BasicRowOper.__init__(self, BRO_ADDMULT, src, k, dst)\n",
    "        self.det_cte = 1\n",
    "        \n",
    "class BroSwap(BasicRowOper):\n",
    "    def __init__(self, src, dst):\n",
    "        BasicRowOper.__init__(self, BRO_SWAP, src, 1, dst)\n",
    "        self.det_cte = -1\n",
    "\n",
    "def make_one(mat, i, j):\n",
    "    det_cte = 1\n",
    "    if mat[i][j] == 0:\n",
    "        nzi = np.nonzero(mat[:,j][i:])[0]\n",
    "        assert nzi.size > 0        \n",
    "        det_cte *= BroSwap(i, nzi[0]).apply(mat)\n",
    "    if mat[i][j] != 1:        \n",
    "        det_cte *= BroMult(1.0/mat[i][j], i).apply(mat)\n",
    "    return det_cte\n",
    "    \n",
    "def make_zero(mat, i, j, pivot):\n",
    "    det_cte = 1\n",
    "    if mat[i][j] != 0:\n",
    "        assert mat[pivot][j] == 1, \\\n",
    "            \"invalid pivot (%d,%d,%d) %f\" % (i, j, pivot, mat[pivot][j])\n",
    "        det_cte *= BroAddMult(pivot, -mat[i][j], i).apply(mat)\n",
    "    return det_cte\n",
    "    \n",
    "def pivot(mat, j, below=True):\n",
    "    n, _ = mat.shape\n",
    "    if below:\n",
    "        row_idx = xrange(j+1, n)\n",
    "    else:\n",
    "        row_idx = xrange(j-1, -1, -1)\n",
    "    det_cte = make_one(mat, j, j)\n",
    "    for i in row_idx:\n",
    "        det_cte *= make_zero(mat, i, j, j)\n",
    "    return det_cte\n",
    "     \n",
    "# returns both the reduced form and det(A)    \n",
    "def gauss_jordan_det(A):\n",
    "    n, m = A.shape\n",
    "    assert n == m\n",
    "    mat = np.empty_like (A)\n",
    "    mat[:] = A\n",
    "    det_cte = 1\n",
    "    for j in xrange(n):\n",
    "        det_cte *= pivot(mat, j)\n",
    "    for j in xrange(n-1, -1, -1):\n",
    "        det_cte *= pivot(mat, j, False)  \n",
    "    det_A = 1/det_cte if det_cte != 0 else 0\n",
    "    return mat, det_A\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our modified routines with same matrix we used on homework one; and compare that with numpy built-in determinant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0, -2.000000, 1, det_cte=1.000000): after\n",
      "[[1 1 1]\n",
      " [0 1 3]\n",
      " [4 0 5]]\n",
      "\n",
      "(2, 0, -4.000000, 2, det_cte=1.000000): after\n",
      "[[ 1  1  1]\n",
      " [ 0  1  3]\n",
      " [ 0 -4  1]]\n",
      "\n",
      "(2, 1, 4.000000, 2, det_cte=1.000000): after\n",
      "[[ 1  1  1]\n",
      " [ 0  1  3]\n",
      " [ 0  0 13]]\n",
      "\n",
      "(1, 2, 0.076923, 2, det_cte=0.076923): after\n",
      "[[1 1 1]\n",
      " [0 1 3]\n",
      " [0 0 1]]\n",
      "\n",
      "(2, 2, -3.000000, 1, det_cte=1.000000): after\n",
      "[[1 1 1]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "(2, 2, -1.000000, 0, det_cte=1.000000): after\n",
      "[[1 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "(2, 1, -1.000000, 0, det_cte=1.000000): after\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "our computed det(A) = 13.000000 \n",
      "numpy det(A) = 13.000000 \n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,1,1], [2,3,5], [4,0,5]])\n",
    "R, det_A = gauss_jordan_det(A)\n",
    "print(\"our computed det(A) = %f \" % det_A)\n",
    "print(\"numpy det(A) = %f \" % np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "<b>Implement the algorithm to obtain inverses from squared matrices using\n",
    "jupyter notebook.</b>\n",
    "\n",
    "In a similar spirit to previous exercise, we will leverage Gauss-Jordan algorithm that we have. We will simply apply same elementary operations against the extended matrix $[A \\;|\\; I]$. At the end, we will have $[I \\;|\\; A^{-1}]$.\n",
    "\n",
    "Below the modified Gauss-Jordan routine (we could integrate the computation of determinant, to know whether inverse actually exist; but just handle non-singular case now due lack of time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to patch the apply function to consider extended matrix\n",
    "def ext_apply(self, mat):\n",
    "    n, m = mat.shape\n",
    "    mat[:,:n] = np.dot(self._basic_matrix(n), mat[:, :n])\n",
    "    mat[:,n:m] = np.dot(self._basic_matrix(n), mat[:, n:m])\n",
    "    print(\"%s: after\\n%s\\n\" % (self, mat))\n",
    "    return self.det_cte\n",
    "    \n",
    "BasicRowOper.apply = ext_apply\n",
    "\n",
    "# returns the inverse of A\n",
    "def gauss_jordan_inv(A):\n",
    "    n, m = A.shape\n",
    "    assert n == m\n",
    "    mat = np.zeros(shape=(n, 2*n))\n",
    "    mat[:, :n] = A\n",
    "    mat[:, n:2*n] = np.identity(n)\n",
    "    for j in xrange(n):\n",
    "        pivot(mat, j)\n",
    "    for j in xrange(n-1, -1, -1):\n",
    "        pivot(mat, j, False) \n",
    "    inv_A = mat[:, n:2*n]\n",
    "    return inv_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simiarly to previous exercise, we test with same matrix and compare the result with numpy's integrated inverse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0, -2.000000, 1, det_cte=1.000000): after\n",
      "[[ 1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1.  3. -2.  1.  0.]\n",
      " [ 4.  0.  5.  0.  0.  1.]]\n",
      "\n",
      "(2, 0, -4.000000, 2, det_cte=1.000000): after\n",
      "[[ 1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1.  3. -2.  1.  0.]\n",
      " [ 0. -4.  1. -4.  0.  1.]]\n",
      "\n",
      "(2, 1, 4.000000, 2, det_cte=1.000000): after\n",
      "[[  1.   1.   1.   1.   0.   0.]\n",
      " [  0.   1.   3.  -2.   1.   0.]\n",
      " [  0.   0.  13. -12.   4.   1.]]\n",
      "\n",
      "(1, 2, 0.076923, 2, det_cte=0.076923): after\n",
      "[[ 1.          1.          1.          1.          0.          0.        ]\n",
      " [ 0.          1.          3.         -2.          1.          0.        ]\n",
      " [ 0.          0.          1.         -0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "(2, 2, -3.000000, 1, det_cte=1.000000): after\n",
      "[[ 1.          1.          1.          1.          0.          0.        ]\n",
      " [ 0.          1.          0.          0.76923077  0.07692308 -0.23076923]\n",
      " [ 0.          0.          1.         -0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "(2, 2, -1.000000, 0, det_cte=1.000000): after\n",
      "[[ 1.          1.          0.          1.92307692 -0.30769231 -0.07692308]\n",
      " [ 0.          1.          0.          0.76923077  0.07692308 -0.23076923]\n",
      " [ 0.          0.          1.         -0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "(2, 1, -1.000000, 0, det_cte=1.000000): after\n",
      "[[ 1.          0.          0.          1.15384615 -0.38461538  0.15384615]\n",
      " [ 0.          1.          0.          0.76923077  0.07692308 -0.23076923]\n",
      " [ 0.          0.          1.         -0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "our computed inv(A) =\n",
      "[[ 1.15384615 -0.38461538  0.15384615]\n",
      " [ 0.76923077  0.07692308 -0.23076923]\n",
      " [-0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "numpy inv(A) =\n",
      "[[ 1.15384615 -0.38461538  0.15384615]\n",
      " [ 0.76923077  0.07692308 -0.23076923]\n",
      " [-0.92307692  0.30769231  0.07692308]]\n",
      "\n",
      "difference between both = \n",
      "[[ -2.22044605e-16   5.55111512e-17   2.77555756e-17]\n",
      " [  1.11022302e-16  -4.16333634e-17   2.77555756e-17]\n",
      " [  0.00000000e+00   0.00000000e+00  -1.38777878e-17]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,1,1], [2,3,5], [4,0,5]])\n",
    "inv_A = gauss_jordan_inv(A)\n",
    "print(\"our computed inv(A) =\\n\" + str(inv_A))\n",
    "print(\"\\nnumpy inv(A) =\\n\"  + str(np.linalg.inv(A)))\n",
    "print(\"\\ndifference between both = \\n\"  + str(inv_A - np.linalg.inv(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
