{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "<b>If a subspace $S$ is contained in a subspace $V$ , prove that $S^\\perp$ contains $V^\\perp$.</b>\n",
    "\n",
    "Let $x \\in V^\\perp$, by definition we know that:\n",
    "\n",
    "$\\forall v \\in V: x^Tv = 0$\n",
    "\n",
    "But since $S \\subset V$, then any property that applies to $V$ also applies to $S$; in particular the statement above. In other words:\n",
    "\n",
    "$(\\forall v \\in V: x^Tv = 0) \\land S \\subset V \\implies \n",
    "\\forall s \\in S: x^Ts = 0$\n",
    "\n",
    "But the right side of above implication is, by definition, the condition for $x \\in S^\\perp$. Therefore, we have proved that:\n",
    "\n",
    "$x \\in V^\\perp \\implies x \\in S^\\perp$\n",
    "\n",
    "And above is the definition of $V^\\perp \\subset S^\\perp$, which is what we wanted to prove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "<b>Find $A^TA$ if the columns of A are unit vectors, all mutually perpendicular.</b>\n",
    "\n",
    "We are describing $A$ as an orthogonal matrix, and we know that there is a theorem stating that:\n",
    "\n",
    "Theorem: Let $A$ be an orthogonal matrix $\\implies$ $A^{-1} = A^T$\n",
    "\n",
    "Applying theorem above we can tell that $A^TA = I$ (identity matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "<b>Suppose I give you four nonzero vectors $r,n,c,l$ in $\\mathbb{R}^2$ </b>\n",
    "\n",
    "#### <b>(a) What are the conditions for those to be bases for the four fundamental sub-spaces $C(A^T), N(A), C(A), N(A^T)$ of a 2x2 matrix.</b>\n",
    "\n",
    "The first thing to realize, is that the four subspaces need to be lines. This comes from the following theorem, relating the dimension of any subspace $V \\subset \\mathbb{R}^n$ and its orthogonal complement $V^{\\perp}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "dim(\\mathbb{R}^n) = n = dim(V) + dim(V^\\perp)\n",
    "\\end{equation*}\n",
    "\n",
    "If we apply the above theorem to the four fundamental subspaces in $\\mathbb{R}^2$, we get the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "dim\\left(C(A^T)\\right) + dim\\left(N(A)\\right) = 2 \\\\\n",
    "dim\\left(C(A)\\right) + dim\\left(N(A^T)\\right) = 2 \n",
    "\\end{equation*}\n",
    "\n",
    "And given that we require to have nonzero vectors on all the bases, then none of them could have dimension zero. The only option left is that the four subspaces have dimension one. This in turn means, that each basis will be formed by a single vector. Without losing generality, let us assume that the vectors are distributed as follows among the subspaces:\n",
    "\n",
    "\\begin{equation*}\n",
    "r \\in C(A^T) \\\\\n",
    "c \\in C(A) \\\\\n",
    "n \\in N(A) \\\\\n",
    "l \\in N(A^T)\n",
    "\\end{equation*}\n",
    "\n",
    "The vectors above are enough to form a basis of their respective subspaces (they are nonzero and their subspaces have dimension one). In particular, given that $dim\\left(C(A^T)\\right) = dim\\left(C(A)\\right) = 1$, then it means the matrix $A$ must have rank 1 as well. And here we leverage following theorem that characterizes such type of matrix:\n",
    "\n",
    "Theorem: Let $A \\in \\mathbb{R}^{n \\times m}$: A is rank 1 $\\iff \\exists v \\in \\mathbb{R}^n, u \\in \\mathbb{R}^m \\ni v\\,u^T = A$\n",
    "\n",
    "The theorem above can be instanciated for $\\mathbb{R}^2$, and dictate the structure of our matrix $A$. There must be vectors $(x,y)$ and $(w,x)$ such that:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "w \\\\\n",
    "z\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x & y\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "wx & wy \\\\\n",
    "zx & zy\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "By looking at the rows of $A$, we realize that both are multiples of $(x,y)$. Similarly, the columns are multiples of $(w, z)$. This brings a natural choice for $r$ and $c$:\n",
    "\n",
    "\\begin{equation*}\n",
    "r = (x, y) \\\\\n",
    "c = (w, z)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, in order to characterize the remaining vectors $n$ and $l$, let us recall that in $\\mathbb{R}^2$ there are only two directions for an orthogonal vector to $(a,b)$: either $(-b, a)$ or $(b,-a)$ (we can use either form, as they are linearly dependent). We also know that $n \\perp r$ and $l \\perp c$; thus, putting it together we can describe the four vectors in terms of 6 parameters in $\\mathbb{R}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "r = (x, y) \\\\\n",
    "c = (w, z) \\\\\n",
    "n = \\alpha\\,(-y, x) \\\\\n",
    "l = \\beta\\,(-z, w) \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "#### (b) What is the possible matrix A?\n",
    "\n",
    "As developed on previous exercise, the matrix $A$ can be described in terms of vectors $r = (x,y)$ and $c = (w,z)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "w \\\\\n",
    "z\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x & y\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "wx & wy \\\\\n",
    "zx & zy\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 \n",
    "<b>The points $P = (x, x, x)$ and $Q = (y, 3y, 1)$ are on two lines in space that do not meet. Choose x and y to minimize the squared distance $\\|P - Q\\|^2$</b>\n",
    "\n",
    "The expectation is to use the tools reviewed in class, hence we should try to project one subspace into the other; because we know that orthogonal projection has an implicit optimization: if we pick one point in one set, and its orthogonal projection into the other, we know that such is the closest pair of points we could ever have between both sets. Once we establish their distance as a function of $x$ and $y$, we proceed to minimize with regular calculus tools. \n",
    "\n",
    "But which subspace shall be projected into the other? Well, first of all let us realize that $Q$ is not a subspace; this is because the $0$ is not there. Since we only know how to project against subspaces, then then only option is to project $Q$ into $P$.\n",
    "\n",
    "In order to build the projection matrix we need an orthonormal basis for $P$, but this is trivial as $P$ is uni-dimensional. A basis can just be $\\left\\{\\frac{1}{\\sqrt(3)}(1, 1, 1)\\right\\}$. \n",
    "\n",
    "Now, the general formula for the projection matrix $P$ is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "P = A\\,(A^T A)^{-1}\\,A^T\n",
    "\\end{equation*}\n",
    "\n",
    "In our case the middle term $(A^T A)^{-1}$ vanishes into identity matrix, cause we chose our basis to be orthonormal. Thus, we end up with a single rank matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "P = A\\,A^T =\n",
    "\\begin{pmatrix}\n",
    "    \\frac{1}{\\sqrt{3}} \\\\ \n",
    "    \\frac{1}{\\sqrt{3}} \\\\\n",
    "    \\frac{1}{\\sqrt{3}} \n",
    "\\end{pmatrix}\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "    \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} \n",
    "\\end{pmatrix} = \n",
    "\\frac{1}{3}\n",
    "\\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Let us apply this projection matrix against an arbitrary element in $Q$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{3}\n",
    "\\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \n",
    "\\end{pmatrix}\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "  y \\\\\n",
    "  3y \\\\\n",
    "  -1\n",
    "\\end{pmatrix} =\n",
    "\\frac{1}{3}\n",
    "\\begin{pmatrix}\n",
    "4y - 1 \\\\\n",
    "4y - 1 \\\\\n",
    "4y - 1 \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Alright, point above represents the orthogonal projection in $P$ of an arbitrary point in $Q$; the reader may realize we are abusing a bit the language by calling those points the same way as the sets they belong to.\n",
    "\n",
    "Let us compute now $\\|P - Q\\|^2$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\|P - Q\\|^2 &= \n",
    "\\left\\|\n",
    "\\frac{1}{3}\n",
    "\\begin{pmatrix}\n",
    "4y - 1 \\\\\n",
    "4y - 1 \\\\\n",
    "4y - 1 \n",
    "\\end{pmatrix} - \n",
    "\\begin{pmatrix}\n",
    "  y \\\\\n",
    "  3y \\\\\n",
    "  -1\n",
    "\\end{pmatrix} \n",
    "\\right\\|^2 \\\\\\\\\n",
    "\\, &= \\left\\|\n",
    "\\begin{pmatrix}\n",
    "\\frac{4y}{3}  - y - \\frac{1}{3} \\\\\n",
    "\\frac{4y}{3}  - 3y - \\frac{1}{3} \\\\\n",
    "\\frac{4y}{3} + 1 - \\frac{1}{3}\n",
    "\\end{pmatrix}  \n",
    "\\right\\|^2 \\\\\\\\\n",
    "\\, &= \\left\\|\n",
    "\\begin{pmatrix}\n",
    "\\frac{y}{3} - \\frac{1}{3} \\\\\n",
    "\\frac{-5y}{3} - \\frac{1}{3} \\\\\n",
    "\\frac{4y}{3} + \\frac{2}{3}\n",
    "\\end{pmatrix}  \n",
    "\\right\\|^2 \\\\\\\\\n",
    "\\, &= \n",
    "\\left(\n",
    "\\frac{1}{3}\n",
    "\\left\\|\n",
    "\\begin{pmatrix}\n",
    "y - 1 \\\\\n",
    "-5y - 1 \\\\\n",
    "4y + 2\n",
    "\\end{pmatrix}  \n",
    "\\right\\|\n",
    "\\right)^2 \\\\\\\\\n",
    "\\, &= \n",
    " \\frac{1}{9}\n",
    "\\left[ (y - 1)^2 + (-5y - 1)^2 + (4y + 2)^2 \\right] \\\\\\\\\n",
    "\\, &= \n",
    " \\frac{1}{9}\n",
    "\\left[ (y^2 -2y + 1) + (25y^2 +10y + 1) + (16y^2 + 16y + 4) \\right] \\\\\\\\\n",
    "\\, &= \n",
    " \\frac{1}{9}\n",
    "\\left( 42y^2 + 24y + 6 \\right) \\\\\\\\\n",
    "\\, &= \n",
    "\\frac{1}{3} \\left( 14y^2 + 8y + 2 \\right) \\\\\\\\\n",
    "\\, &= \n",
    "F(y)\n",
    "\\end{align*}\n",
    "\n",
    "A nice consequence of having using orthogonal projections, is that the distance function $D(y)$ only depends on $y$. Let us now derive and get the minimum:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dF(y)}{dy} = \\frac{1}{3}(28y + 8) = \\frac{2}{3}(14y + 4) & \\implies \\\\\n",
    "\\frac{dF(y)}{dy} = 0 \\iff y = \\frac{-4}{14} = -\\frac{2}{7}\n",
    "\\end{align*}\n",
    "\n",
    "Since the second derivative of $D(y)$ is just positive (28), we know it reaches a minimum on the critical point $y = -\\frac{2}{7}$. Per our previous formulation of the orthogonal projection, we know what will be $x$\n",
    "\n",
    "\\begin{align*}\n",
    "x  &= \\frac{1}{3} (4y - 1) \\\\\n",
    "\\, &= \\frac{1}{3} \\left(4\\left(\\frac{-2}{7}\\right) - 1\\right) \\\\\n",
    "\\, &= \\frac{1}{3} \\left(\\frac{-15}{7}\\right) \\\\\n",
    "\\, &= -\\frac{5}{7}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we reach the minimum distance $\\|P - Q\\|^2 = \\frac{2}{7}$ on $x = -\\frac{5}{7}$ and $y = -\\frac{2}{7}$.\n",
    "\n",
    "#### <b>(a) What is the perpendicular line connecting the closest P and Q?</b>\n",
    "\n",
    "This perpendicular line will pass precisely through the points $P$ and $Q$ that we found in previous exercise. Given a couple of points in $\\mathbb{R}^n$, the parametric equation of the line is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "P + \\lambda(P - Q)  \\;\\;\\forall \\lambda \\in \\mathbb{R}\n",
    "\\end{equation*}\n",
    "\n",
    "We just need to use the concrete values we have for $P$ and $Q$ (given that we know $x$ and $y$):\n",
    "\n",
    "\\begin{align*}\n",
    "P + \\lambda(P - Q) &= \\\\\n",
    "\\, &= (x,x,x) + \\lambda\\left[(x,x,x) - (y,3y,-1)\\right] \\\\\n",
    "\\, &= (\\frac{-5}{7},\\frac{-5}{7},\\frac{-5}{7}) + \\lambda\\left[(\\frac{-5}{7},\\frac{-5}{7},\\frac{-5}{7}) - (\\frac{-2}{7},\\frac{-6}{7},-1)\\right] \\\\\n",
    "\\, &= (\\frac{-5}{7},\\frac{-5}{7},\\frac{-5}{7}) + \\lambda(\\frac{-3}{7},\\frac{1}{7},\\frac{2}{7}) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "<b>Given the data set provided implement the Least Squared Error discussed\n",
    "in class and answer the following questions:\n",
    "\n",
    "#### 5.1 What is the probability of error and the probability of correct classification by simply using the rules $w^T\\,x > 0$ and $w^T\\,x < 0$.</b>\n",
    "\n",
    "Let us bring first the data with Pandas. We do not know what the data is, but looks like a dataset for supervised learning; there are vectors with around twenty features and a label (last column), telling if the row should be classified as male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "5     0.132786  0.079557  0.119090  0.067958  0.209592  0.141634   1.932562   \n",
      "6     0.150762  0.074463  0.160106  0.092899  0.205718  0.112819   1.530643   \n",
      "7     0.160514  0.076767  0.144337  0.110532  0.231962  0.121430   1.397156   \n",
      "8     0.142239  0.078018  0.138587  0.088206  0.208587  0.120381   1.099746   \n",
      "9     0.134329  0.080350  0.121451  0.075580  0.201957  0.126377   1.190368   \n",
      "10    0.157021  0.071943  0.168160  0.101430  0.216740  0.115310   0.979442   \n",
      "11    0.138551  0.077054  0.127527  0.087314  0.202739  0.115426   1.626770   \n",
      "12    0.137343  0.080877  0.124263  0.083145  0.209227  0.126082   1.378728   \n",
      "13    0.181225  0.060042  0.190953  0.128839  0.229532  0.100693   1.369430   \n",
      "14    0.183115  0.066982  0.191233  0.129149  0.240152  0.111004   3.568104   \n",
      "15    0.174272  0.069411  0.190874  0.115602  0.228279  0.112677   4.485038   \n",
      "16    0.190846  0.065790  0.207951  0.132280  0.244357  0.112076   1.562304   \n",
      "17    0.171247  0.074872  0.152807  0.122391  0.243617  0.121227   3.207170   \n",
      "18    0.168346  0.074121  0.145618  0.115756  0.239824  0.124068   2.704335   \n",
      "19    0.173631  0.073352  0.153569  0.123680  0.244234  0.120554   2.804975   \n",
      "20    0.172754  0.076903  0.177736  0.120070  0.245368  0.125298   2.967765   \n",
      "21    0.181015  0.074369  0.169299  0.128673  0.254175  0.125502   2.587325   \n",
      "22    0.163536  0.072449  0.145543  0.113930  0.227449  0.113519   3.587650   \n",
      "23    0.170213  0.075105  0.146053  0.123989  0.250126  0.126137   2.816793   \n",
      "24    0.160422  0.076615  0.144824  0.120924  0.237244  0.116319   6.253208   \n",
      "25    0.164700  0.075362  0.147018  0.118698  0.240475  0.121777   4.208608   \n",
      "26    0.169579  0.075635  0.186468  0.116706  0.238549  0.121843   4.269923   \n",
      "27    0.169021  0.071778  0.143168  0.125801  0.248315  0.122515   3.079273   \n",
      "28    0.167340  0.072841  0.141739  0.122174  0.240000  0.117826   2.192126   \n",
      "29    0.180528  0.070867  0.142385  0.129541  0.252477  0.122936   2.799969   \n",
      "...        ...       ...       ...       ...       ...       ...        ...   \n",
      "3138  0.114477  0.081973  0.090199  0.041095  0.199900  0.158806   1.103680   \n",
      "3139  0.112769  0.074424  0.094248  0.049183  0.183235  0.134052   0.945953   \n",
      "3140  0.126439  0.079412  0.127325  0.046889  0.198993  0.152103   1.452173   \n",
      "3141  0.117350  0.090035  0.109478  0.024017  0.203946  0.179929   2.610623   \n",
      "3142  0.104793  0.085201  0.077886  0.028388  0.186101  0.157712   2.419127   \n",
      "3143  0.127633  0.084931  0.158892  0.034531  0.201430  0.166899   1.591174   \n",
      "3144  0.091250  0.086956  0.048191  0.015193  0.179043  0.163851   3.089787   \n",
      "3145  0.082404  0.085136  0.035114  0.016920  0.152827  0.135906   2.570944   \n",
      "3146  0.124695  0.080989  0.131882  0.042033  0.197268  0.155234   1.970756   \n",
      "3147  0.131566  0.084354  0.131889  0.053093  0.196147  0.143055   2.243370   \n",
      "3148  0.108888  0.092021  0.070063  0.022520  0.201180  0.178660   2.235435   \n",
      "3149  0.090445  0.079045  0.059358  0.020893  0.167727  0.146834   2.187161   \n",
      "3150  0.137507  0.091521  0.161298  0.043547  0.221260  0.177713   1.119608   \n",
      "3151  0.113148  0.090335  0.084335  0.026622  0.198830  0.172207   2.258273   \n",
      "3152  0.149731  0.082852  0.180932  0.060212  0.219788  0.159576   1.240037   \n",
      "3153  0.189614  0.035933  0.194116  0.168434  0.205289  0.036855   2.724415   \n",
      "3154  0.200097  0.045533  0.203796  0.176581  0.232133  0.055552   1.160197   \n",
      "3155  0.178573  0.046679  0.164388  0.149309  0.204601  0.055293   3.066668   \n",
      "3156  0.201806  0.036057  0.201622  0.178165  0.227872  0.049707   1.585353   \n",
      "3157  0.203627  0.041529  0.204104  0.175661  0.239122  0.063461   1.462972   \n",
      "3158  0.183667  0.040607  0.182534  0.156480  0.207646  0.051166   2.054138   \n",
      "3159  0.168794  0.085842  0.188980  0.095558  0.240229  0.144671   1.462248   \n",
      "3160  0.151771  0.089147  0.185970  0.058159  0.230199  0.172040   1.227710   \n",
      "3161  0.170656  0.081237  0.184277  0.113012  0.239096  0.126084   1.378256   \n",
      "3162  0.146023  0.092525  0.183434  0.041747  0.224337  0.182590   1.384981   \n",
      "3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n",
      "3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n",
      "3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n",
      "3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n",
      "3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n",
      "\n",
      "             kurt    sp.ent       sfm   ...    centroid   meanfun    minfun  \\\n",
      "0      274.402906  0.893369  0.491918   ...    0.059781  0.084279  0.015702   \n",
      "1      634.613855  0.892193  0.513724   ...    0.066009  0.107937  0.015826   \n",
      "2     1024.927705  0.846389  0.478905   ...    0.077316  0.098706  0.015656   \n",
      "3        4.177296  0.963322  0.727232   ...    0.151228  0.088965  0.017798   \n",
      "4        4.333713  0.971955  0.783568   ...    0.135120  0.106398  0.016931   \n",
      "5        8.308895  0.963181  0.738307   ...    0.132786  0.110132  0.017112   \n",
      "6        5.987498  0.967573  0.762638   ...    0.150762  0.105945  0.026230   \n",
      "7        4.766611  0.959255  0.719858   ...    0.160514  0.093052  0.017758   \n",
      "8        4.070284  0.970723  0.770992   ...    0.142239  0.096729  0.017957   \n",
      "9        4.787310  0.975246  0.804505   ...    0.134329  0.105881  0.019300   \n",
      "10       3.974223  0.965249  0.733693   ...    0.157021  0.088894  0.022069   \n",
      "11       6.291365  0.966004  0.752042   ...    0.138551  0.104199  0.019139   \n",
      "12       5.008952  0.963514  0.736150   ...    0.137343  0.092644  0.016789   \n",
      "13       5.475600  0.937446  0.537080   ...    0.181225  0.131504  0.025000   \n",
      "14      35.384748  0.940333  0.571394   ...    0.183115  0.102799  0.020833   \n",
      "15      61.764908  0.950972  0.635199   ...    0.174272  0.102046  0.018328   \n",
      "16       7.834350  0.938546  0.538810   ...    0.190846  0.113323  0.017544   \n",
      "17      25.765565  0.936954  0.586420   ...    0.171247  0.079718  0.015671   \n",
      "18      18.484703  0.934523  0.559742   ...    0.168346  0.083484  0.015717   \n",
      "19      20.857543  0.930917  0.518269   ...    0.173631  0.090130  0.015702   \n",
      "20      20.078115  0.925539  0.523081   ...    0.172754  0.093574  0.015764   \n",
      "21      12.281432  0.915284  0.475317   ...    0.181015  0.098643  0.016145   \n",
      "22      28.653781  0.927015  0.542422   ...    0.163536  0.062542  0.015686   \n",
      "23      13.764582  0.913832  0.487966   ...    0.170213  0.077698  0.015702   \n",
      "24      85.491926  0.933030  0.567424   ...    0.160422  0.098944  0.016097   \n",
      "25      43.681885  0.940669  0.604020   ...    0.164700  0.082963  0.015640   \n",
      "26      45.895248  0.929498  0.543709   ...    0.169579  0.082451  0.016211   \n",
      "27      14.340299  0.902275  0.477746   ...    0.169021  0.130598  0.015842   \n",
      "28       8.152410  0.913763  0.539479   ...    0.167340  0.120052  0.016244   \n",
      "29      12.190361  0.853115  0.313426   ...    0.180528  0.126607  0.017039   \n",
      "...           ...       ...       ...   ...         ...       ...       ...   \n",
      "3138     3.759184  0.954130  0.689347   ...    0.114477  0.189394  0.016343   \n",
      "3139     3.290904  0.965719  0.742464   ...    0.112769  0.169836  0.017917   \n",
      "3140     5.582106  0.971946  0.790175   ...    0.126439  0.179613  0.029575   \n",
      "3141    12.442898  0.953624  0.701434   ...    0.117350  0.178040  0.016512   \n",
      "3142    11.281968  0.956977  0.718462   ...    0.104793  0.183565  0.016444   \n",
      "3143     5.347645  0.949757  0.671247   ...    0.127633  0.178363  0.058182   \n",
      "3144    12.857558  0.930715  0.622816   ...    0.091250  0.170893  0.016178   \n",
      "3145     9.179264  0.921649  0.576089   ...    0.082404  0.183387  0.034043   \n",
      "3146     8.000504  0.958531  0.721682   ...    0.124695  0.182513  0.068966   \n",
      "3147    11.544740  0.968324  0.784108   ...    0.131566  0.191163  0.029144   \n",
      "3148     8.528681  0.947621  0.679795   ...    0.108888  0.160473  0.019512   \n",
      "3149     8.221164  0.942404  0.628992   ...    0.090445  0.182431  0.026622   \n",
      "3150     4.185207  0.967706  0.739008   ...    0.137507  0.190093  0.019116   \n",
      "3151     9.579337  0.957433  0.717683   ...    0.113148  0.187444  0.023495   \n",
      "3152     4.019385  0.949787  0.652936   ...    0.149731  0.183974  0.051948   \n",
      "3153    10.986864  0.871215  0.236684   ...    0.189614  0.163059  0.029685   \n",
      "3154     3.733815  0.919607  0.357144   ...    0.200097  0.168531  0.063241   \n",
      "3155    15.684088  0.891448  0.321169   ...    0.178573  0.155380  0.025478   \n",
      "3156     4.945634  0.884731  0.227903   ...    0.201806  0.191704  0.032720   \n",
      "3157     4.790370  0.903458  0.246953   ...    0.203627  0.146783  0.020566   \n",
      "3158     7.483019  0.898138  0.313925   ...    0.183667  0.149237  0.018648   \n",
      "3159     5.077956  0.956201  0.706861   ...    0.168794  0.182863  0.020699   \n",
      "3160     4.304354  0.962045  0.744590   ...    0.151771  0.201600  0.023426   \n",
      "3161     5.431663  0.950750  0.658558   ...    0.170656  0.198475  0.160000   \n",
      "3162     5.118927  0.948999  0.659825   ...    0.146023  0.195640  0.039506   \n",
      "3163     6.630383  0.962934  0.763182   ...    0.131884  0.182790  0.083770   \n",
      "3164     2.503954  0.960716  0.709570   ...    0.116221  0.188980  0.034409   \n",
      "3165     6.604509  0.946854  0.654196   ...    0.142056  0.209918  0.039506   \n",
      "3166     5.388298  0.950436  0.675470   ...    0.143659  0.172375  0.034483   \n",
      "3167     5.769115  0.938829  0.601529   ...    0.165509  0.185607  0.062257   \n",
      "\n",
      "        maxfun   meandom    mindom    maxdom   dfrange   modindx   label  \n",
      "0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000    male  \n",
      "1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632    male  \n",
      "2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512    male  \n",
      "3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119    male  \n",
      "4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274    male  \n",
      "5     0.253968  0.298222  0.007812  2.726562  2.718750  0.125160    male  \n",
      "6     0.266667  0.479620  0.007812  5.312500  5.304688  0.123992    male  \n",
      "7     0.144144  0.301339  0.007812  0.539062  0.531250  0.283937    male  \n",
      "8     0.250000  0.336476  0.007812  2.164062  2.156250  0.148272    male  \n",
      "9     0.262295  0.340365  0.015625  4.695312  4.679688  0.089920    male  \n",
      "10    0.117647  0.460227  0.007812  2.812500  2.804688  0.200000    male  \n",
      "11    0.262295  0.246094  0.007812  2.718750  2.710938  0.132351    male  \n",
      "12    0.213333  0.481671  0.015625  5.015625  5.000000  0.088500    male  \n",
      "13    0.275862  1.277114  0.007812  2.804688  2.796875  0.416550    male  \n",
      "14    0.275862  1.245739  0.203125  6.742188  6.539062  0.139332    male  \n",
      "15    0.246154  1.621299  0.007812  7.000000  6.992188  0.209311    male  \n",
      "16    0.275862  1.434115  0.007812  6.320312  6.312500  0.254780    male  \n",
      "17    0.262295  0.106279  0.007812  0.570312  0.562500  0.138355    male  \n",
      "18    0.231884  0.146563  0.007812  3.125000  3.117188  0.059537    male  \n",
      "19    0.210526  0.193044  0.007812  2.820312  2.812500  0.068124    male  \n",
      "20    0.200000  0.235877  0.007812  0.718750  0.710938  0.235069    male  \n",
      "21    0.275862  0.209844  0.007812  3.695312  3.687500  0.059940    male  \n",
      "22    0.197531  0.059622  0.007812  0.445312  0.437500  0.091699    male  \n",
      "23    0.192771  0.101562  0.007812  0.562500  0.554688  0.161791    male  \n",
      "24    0.275862  0.206756  0.007812  3.953125  3.945312  0.073890    male  \n",
      "25    0.253968  0.143353  0.007812  1.062500  1.054688  0.125926    male  \n",
      "26    0.271186  0.148438  0.007812  3.609375  3.601562  0.050841    male  \n",
      "27    0.225352  0.335313  0.007812  0.710938  0.703125  0.397354    male  \n",
      "28    0.262295  0.298678  0.007812  0.679688  0.671875  0.384778    male  \n",
      "29    0.177778  0.234863  0.007812  0.507812  0.500000  0.329241    male  \n",
      "...        ...       ...       ...       ...       ...       ...     ...  \n",
      "3138  0.271186  0.566840  0.007812  4.273438  4.265625  0.183258  female  \n",
      "3139  0.262295  0.877604  0.007812  4.937500  4.929688  0.171708  female  \n",
      "3140  0.266667  0.614110  0.007812  4.914062  4.906250  0.090045  female  \n",
      "3141  0.271186  0.188721  0.007812  0.750000  0.742188  0.277759  female  \n",
      "3142  0.275862  0.297953  0.007812  0.859375  0.851562  0.370904  female  \n",
      "3143  0.271186  0.634014  0.015625  5.031250  5.015625  0.121246  female  \n",
      "3144  0.275862  0.767314  0.007812  5.289062  5.281250  0.187912  female  \n",
      "3145  0.275862  0.328962  0.007812  0.750000  0.742188  0.445053  female  \n",
      "3146  0.238806  0.293527  0.007812  0.851562  0.843750  0.396091  female  \n",
      "3147  0.275862  0.214725  0.007812  0.796875  0.789062  0.351645  female  \n",
      "3148  0.275862  0.497721  0.007812  2.945312  2.937500  0.236240  female  \n",
      "3149  0.258065  0.735453  0.007812  5.531250  5.523438  0.170489  female  \n",
      "3150  0.275862  0.354367  0.007812  3.117188  3.109375  0.096069  female  \n",
      "3151  0.262295  0.622489  0.007812  4.898438  4.890625  0.128717  female  \n",
      "3152  0.253968  1.361213  0.203125  6.031250  5.828125  0.365700  female  \n",
      "3153  0.258065  1.370192  0.164062  7.000000  6.835938  0.235948  female  \n",
      "3154  0.262295  0.718750  0.148438  7.000000  6.851562  0.092208  female  \n",
      "3155  0.253968  0.637921  0.148438  6.148438  6.000000  0.101291  female  \n",
      "3156  0.275862  0.593750  0.007812  5.921875  5.914062  0.124383  female  \n",
      "3157  0.262295  0.875558  0.171875  6.898438  6.726562  0.145534  female  \n",
      "3158  0.262295  0.550312  0.007812  3.421875  3.414062  0.166503  female  \n",
      "3159  0.271186  0.988281  0.007812  5.882812  5.875000  0.268617  female  \n",
      "3160  0.266667  0.766741  0.007812  4.007812  4.000000  0.192220  female  \n",
      "3161  0.253968  0.414062  0.007812  0.734375  0.726562  0.336918  female  \n",
      "3162  0.275862  0.533854  0.007812  2.992188  2.984375  0.258924  female  \n",
      "3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929  female  \n",
      "3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897  female  \n",
      "3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759  female  \n",
      "3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002  female  \n",
      "3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000  female  \n",
      "\n",
      "[3168 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_url = \"https://gitlab.com/dario.mx/data-science-learning/raw/e322badaee9ce80d732fdeb119ec8699c201cc21/machine-learning/course-math-for-ai/hw3/voice.csv.gz\"\n",
    "df = pd.read_csv(dataset_url, compression='gzip')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let us generate the matrix of inputs $A$ and the expected answer $b$. For the former, $A$, we just need to add a column of ones at the beginning (this accounts for the translation factor in our linear model).\n",
    "\n",
    "For the vector $b$, we will replace the male/female labels in data set by 1/-1 codes. The default encoding of Pandas will use 1/0, but we do not want 0 as our discriminator function will be the sign of the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.05978098  0.06424127 ...,  0.0078125   0.          0.        ]\n",
      " [ 1.          0.06600874  0.06731003 ...,  0.0546875   0.046875\n",
      "   0.05263158]\n",
      " [ 1.          0.0773155   0.08382942 ...,  0.015625    0.0078125\n",
      "   0.04651163]\n",
      " ..., \n",
      " [ 1.          0.14205626  0.09579843 ...,  2.9375      2.9296875\n",
      "   0.19475862]\n",
      " [ 1.          0.14365874  0.09062826 ...,  3.59375     3.5859375\n",
      "   0.31100218]\n",
      " [ 1.          0.16550895  0.09288354 ...,  0.5546875   0.546875    0.35      ]]\n",
      "[ 1  1  1 ..., -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = df.select_dtypes(include=['floating']).as_matrix()\n",
    "A = np.c_[np.ones(A.shape[0]), A]\n",
    "b = df.label.astype('category').cat.codes.as_matrix().copy()\n",
    "b[b == 0] = -1\n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the parameters $w$ according to the least square error formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "    w = (A^T\\,A)^{-1}\\,A^T\\,b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -7.14083583e-01   4.01824932e+13   3.53148711e+02  -7.89516607e+01\n",
      "  -8.83435086e+13   8.83435086e+13  -8.83435086e+13  -5.43889098e-01\n",
      "   1.18898381e-02  -1.12456556e+01  -7.04423828e+00   2.82032480e+00\n",
      "  -4.01824932e+13  -3.42171237e+01   8.20218274e+00   3.37094456e+00\n",
      "   1.03277442e-02  -3.85639667e-02   5.89086925e-01  -5.83150256e-01\n",
      "   4.84137019e-02]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array, dot\n",
    "from numpy.linalg import inv\n",
    "\n",
    "w = dot(dot(inv(dot(A.T, A)), A.T), b)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the values $w^T\\,x$ for every row $x$ on $A$, then we map the sign of the result into a category, and finally we just compare with the expected values to get the number of right predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_prob=0.775884, error_prob=0.224116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = np.sign(dot(A, w))\n",
    "n = float(b.shape[0])\n",
    "accuracy_prob = np.sum(pred == b) / n\n",
    "error_prob = 1 - accuracy_prob\n",
    "print(\"accuracy_prob=%f, error_prob=%f\\n\" % (accuracy_prob, error_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got an accuracy of 77%, not bad at all; but we can get even better results if we pay more attention to the calculations done.\n",
    "\n",
    "We used the formula $w = (A^T\\,A)^{-1}\\,A^T\\,b$, which assumes that $(A^T\\,A)$ is invertible. Even if the numpy routine $inv$ does not raise an error, it does return something suspicious. Let us take a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.31235137e+00,   4.58052429e+01,  -9.80983566e-01,\n",
       "          7.29750732e-01,  -6.30725529e+00,   8.00240196e+00,\n",
       "         -6.82179969e+00,  -1.56145251e-02,   3.39131388e-04,\n",
       "         -1.29683913e+00,   1.87301511e-01,  -2.09387244e-02,\n",
       "         -4.86473869e+01,  -4.14960507e-01,  -7.11653426e-02,\n",
       "          1.58351315e-02,   6.04113572e-03,  -2.18440887e-01,\n",
       "          1.91777315e-01,  -1.93747795e-01,  -1.70341082e-02],\n",
       "       [  2.45699213e+01,   3.16079498e+14,  -8.49143982e+01,\n",
       "          5.84374263e+01,  -3.02527745e+12,   3.02527745e+12,\n",
       "         -3.02527745e+12,  -2.44351197e-01,   5.88710719e-03,\n",
       "         -1.71656500e+01,   1.38388322e+00,  -8.09627738e-01,\n",
       "         -3.17336183e+14,  -4.09567624e+00,   8.12758082e-01,\n",
       "          1.45727068e-01,   2.56629249e-01,  -3.55990620e+12,\n",
       "          3.55990620e+12,  -3.55990620e+12,  -1.63809267e-01],\n",
       "       [ -9.80983566e-01,  -3.14909417e+01,   2.29236591e+01,\n",
       "         -5.53740349e+00,   1.65710827e+01,  -2.93793471e+01,\n",
       "          1.72965807e+01,  -2.67435806e-02,   6.40528534e-04,\n",
       "          1.26629645e-01,  -5.65413792e-01,   8.77185294e-02,\n",
       "          4.12795068e+01,  -2.22506263e-01,   1.58469437e-01,\n",
       "         -2.14956686e-01,   5.77326430e-03,  -3.78435126e-01,\n",
       "          5.37584806e-01,  -5.34660357e-01,  -3.51925740e-02],\n",
       "       [  7.29750732e-01,   4.03701298e+01,  -5.53740349e+00,\n",
       "          4.34855322e+00,  -8.41566936e+00,   1.49126294e+01,\n",
       "         -1.03470304e+01,  -1.31441804e-03,   5.18925876e-06,\n",
       "         -1.89433967e-01,  -1.17060134e-02,  -8.89156587e-02,\n",
       "         -5.07494096e+01,  -1.83800684e-01,   1.17662228e-01,\n",
       "          8.69982133e-02,   9.32973738e-03,   1.19399240e-01,\n",
       "         -1.53886899e-01,   1.53227794e-01,   4.93899671e-03],\n",
       "       [  2.73221328e+01,   3.37459262e+14,  -1.06010451e+02,\n",
       "          7.45477440e+01,   6.38768858e+13,  -6.38768858e+13,\n",
       "          6.38768858e+13,  -2.96546568e-01,   7.38036748e-03,\n",
       "         -1.67204338e+01,   2.75548364e-01,  -4.45628626e-01,\n",
       "         -3.34696369e+14,  -3.64218857e+00,   8.18589798e-01,\n",
       "          3.25007822e-01,   2.99616997e-01,  -4.06088673e+12,\n",
       "          4.06088673e+12,  -4.06088673e+12,  -6.05358868e-02],\n",
       "       [ -2.56272434e+01,  -3.37459262e+14,   9.32032848e+01,\n",
       "         -6.80446061e+01,  -6.38768858e+13,   6.38768858e+13,\n",
       "         -6.38768858e+13,   2.93962684e-01,  -7.34391560e-03,\n",
       "          1.60808523e+01,  -3.40955375e-01,   5.60165456e-01,\n",
       "          3.34696369e+14,   3.32412043e+00,  -5.94503328e-01,\n",
       "         -1.96868318e-01,  -2.78389373e-01,   4.06088673e+12,\n",
       "         -4.06088673e+12,   4.06088673e+12,   6.68818034e-02],\n",
       "       [  2.68079483e+01,   3.37459262e+14,  -1.05277511e+02,\n",
       "          7.25946910e+01,   6.38768858e+13,  -6.38768858e+13,\n",
       "          6.38768858e+13,  -2.86554152e-01,   7.09892164e-03,\n",
       "         -1.65955818e+01,   4.15617118e-01,  -5.12371155e-01,\n",
       "         -3.34696369e+14,  -3.30580677e+00,   5.71199429e-01,\n",
       "          2.60850387e-01,   2.88945526e-01,  -4.06088673e+12,\n",
       "          4.06088673e+12,  -4.06088673e+12,  -5.63879702e-02],\n",
       "       [ -1.56145251e-02,  -6.81851088e-01,  -2.67435806e-02,\n",
       "         -1.31441804e-03,   2.35912229e-02,  -2.61751069e-02,\n",
       "          3.35836392e-02,   7.18566973e-04,  -2.00071664e-05,\n",
       "          1.48631964e-02,  -7.25527879e-04,   1.27954920e-03,\n",
       "          7.06206056e-01,   4.31038033e-03,   1.32147073e-03,\n",
       "         -2.30403538e-04,   6.83180642e-05,   4.60541990e-03,\n",
       "         -5.27886146e-03,   5.29404857e-03,  -7.34688183e-05],\n",
       "       [  3.39131388e-04,   1.56101416e-02,   6.40528534e-04,\n",
       "          5.18925877e-06,  -2.29706731e-04,   2.66158615e-04,\n",
       "         -5.11152579e-04,  -2.00071664e-05,   5.95984711e-07,\n",
       "         -3.10842588e-04,   1.39523603e-05,  -2.11572282e-05,\n",
       "         -1.61244718e-02,  -8.96194834e-05,  -3.02189055e-05,\n",
       "          1.17648096e-06,  -3.00306630e-06,  -1.14493913e-04,\n",
       "          1.30621896e-04,  -1.30683033e-04,   1.41252728e-05],\n",
       "       [ -1.29683913e+00,  -4.27985611e+01,   1.26629645e-01,\n",
       "         -1.89433967e-01,   5.72353997e+00,  -6.36312145e+00,\n",
       "          5.84839204e+00,   1.48631964e-02,  -3.10842588e-04,\n",
       "          1.51046011e+00,  -2.72088072e-01,   2.78492471e-02,\n",
       "          4.38871761e+01,   4.42715508e-01,   9.95644899e-02,\n",
       "         -1.31334637e-01,  -1.88326479e-03,   2.35717371e-01,\n",
       "         -2.33749204e-01,   2.34926377e-01,  -6.55192335e-04],\n",
       "       [  1.87301511e-01,   4.25921372e+00,  -5.65413792e-01,\n",
       "         -1.17060134e-02,  -1.12984768e+00,   1.06444067e+00,\n",
       "         -9.89778926e-01,  -7.25527879e-04,   1.39523603e-05,\n",
       "         -2.72088072e-01,   1.16696314e-01,  -1.35934306e-02,\n",
       "         -3.76621271e+00,  -6.89792101e-02,   8.43505954e-04,\n",
       "          2.97953467e-02,  -7.93404017e-04,  -2.42402177e-02,\n",
       "          2.44678638e-02,  -2.46519812e-02,  -2.20601500e-04],\n",
       "       [ -2.09387244e-02,  -6.93969277e-01,   8.77185294e-02,\n",
       "         -8.89156587e-02,   2.80077067e-01,  -1.65540237e-01,\n",
       "          2.13334538e-01,   1.27954920e-03,  -2.11572282e-05,\n",
       "          2.78492471e-02,  -1.35934306e-02,   1.31064495e-01,\n",
       "          4.38380765e-01,   1.48898747e-03,  -5.54116471e-02,\n",
       "          2.21608923e-02,  -1.50106969e-03,  -1.13270787e-02,\n",
       "          6.55025087e-04,  -6.54868649e-04,   1.02738150e-02],\n",
       "       [ -2.75559067e+01,  -3.16079498e+14,   1.05625268e+02,\n",
       "         -7.13388541e+01,   3.02527745e+12,  -3.02527745e+12,\n",
       "          3.02527745e+12,   2.53699617e-01,  -6.05833069e-03,\n",
       "          1.79390106e+01,  -1.08701553e+00,   6.11404675e-01,\n",
       "          3.17336183e+14,   4.58339570e+00,  -1.21183185e+00,\n",
       "         -4.64451294e-01,  -3.01682870e-01,   3.55990620e+12,\n",
       "         -3.55990620e+12,   3.55990620e+12,   1.64026647e-01],\n",
       "       [ -4.14960507e-01,  -9.54046007e+00,  -2.22506263e-01,\n",
       "         -1.83800684e-01,   2.20793428e+00,  -2.52600242e+00,\n",
       "          2.54431608e+00,   4.31038033e-03,  -8.96194834e-05,\n",
       "          4.42715508e-01,  -6.89792101e-02,   1.48898747e-03,\n",
       "          1.03231961e+01,   6.54916124e-01,  -1.35850853e-01,\n",
       "         -1.90236138e-01,   2.33506301e-03,   4.55972693e-02,\n",
       "         -5.65366293e-02,   5.64395141e-02,  -1.21207878e-02],\n",
       "       [ -7.11653426e-02,  -4.87057272e+00,   1.58469437e-01,\n",
       "          1.17662228e-01,  -4.71851878e-01,   6.95938348e-01,\n",
       "         -7.19242246e-01,   1.32147073e-03,  -3.02189055e-05,\n",
       "          9.95644899e-02,   8.43505954e-04,  -5.54116471e-02,\n",
       "          4.39992030e+00,  -1.35850853e-01,   1.21088089e+00,\n",
       "         -5.98042110e-02,  -5.79859310e-03,   7.19961091e-02,\n",
       "         -5.97775417e-02,   5.96943206e-02,  -3.16384770e-02],\n",
       "       [  1.58351315e-02,  -1.40280288e+00,  -2.14956686e-01,\n",
       "          8.69982133e-02,   4.04604793e-02,   8.76790251e-02,\n",
       "         -2.36969562e-02,  -2.30403538e-04,   1.17648096e-06,\n",
       "         -1.31334637e-01,   2.97953467e-02,   2.21608923e-02,\n",
       "          1.02378188e+00,  -1.90236138e-01,  -5.98042110e-02,\n",
       "          5.50667784e-01,  -7.99898230e-03,   3.33977373e-02,\n",
       "          2.06514956e-02,  -2.04600906e-02,   3.82269733e-02],\n",
       "       [  6.04113572e-03,   4.09554829e-01,   5.77326430e-03,\n",
       "          9.32973738e-03,  -4.24683573e-02,   6.36959813e-02,\n",
       "         -5.31398289e-02,   6.83180642e-05,  -3.00306630e-06,\n",
       "         -1.88326479e-03,  -7.93404017e-04,  -1.50106969e-03,\n",
       "         -4.59078093e-01,   2.33506301e-03,  -5.79859310e-03,\n",
       "         -7.99898230e-03,   4.28434927e-03,  -1.92475712e-03,\n",
       "          2.05659066e-04,  -6.95195061e-04,  -4.24392304e-03],\n",
       "       [ -2.29678970e-02,  -4.39804651e+12,   1.54020189e-01,\n",
       "         -3.43819726e-02,   2.36576495e-01,  -2.63237569e-01,\n",
       "          2.49234499e-01,  -6.92253740e-04,   1.14578674e-05,\n",
       "         -9.54158134e-04,   8.26940870e-04,  -1.05289131e-02,\n",
       "          4.39804651e+12,  -1.07467134e-02,   1.17831899e-02,\n",
       "          5.41307746e-02,  -7.04139826e-04,   1.01014570e-01,\n",
       "          6.27642669e-03,  -6.07799991e-03,  -1.09647300e-02],\n",
       "       [ -3.32879267e-03,   4.39804651e+12,   4.28075228e-03,\n",
       "          7.80509097e-04,   1.03812214e-02,  -9.67601911e-03,\n",
       "          9.71977690e-03,   1.88121880e-05,   4.67011543e-06,\n",
       "          2.92232522e-03,  -5.99294723e-04,  -1.43140536e-04,\n",
       "         -4.39804651e+12,  -1.92646584e-04,   4.35377484e-04,\n",
       "         -8.15416200e-05,  -1.01495823e-03,   1.47852575e-03,\n",
       "         -9.75423723e-04,   1.02421152e-03,   1.99326606e-03],\n",
       "       [  1.86968432e-03,  -4.39804651e+12,  -2.78405549e-03,\n",
       "         -4.02596260e-04,  -5.89564692e-03,   5.51352166e-03,\n",
       "         -5.56811098e-03,  -3.62507225e-06,  -4.73125239e-06,\n",
       "         -1.74515243e-03,   4.15177393e-04,   1.43296974e-04,\n",
       "          4.39804651e+12,   9.55313159e-05,  -5.18598572e-04,\n",
       "          2.72946617e-04,   5.25422237e-04,  -8.46134512e-04,\n",
       "          5.90247059e-04,  -5.32245903e-04,  -1.00990248e-03],\n",
       "       [ -1.70341082e-02,  -8.39960024e-01,  -3.51925740e-02,\n",
       "          4.93899671e-03,   5.33113857e-02,  -4.69654691e-02,\n",
       "          5.74593023e-02,  -7.34688183e-05,   1.41252728e-05,\n",
       "         -6.55192335e-04,  -2.20601500e-04,   1.02738150e-02,\n",
       "          8.40177404e-01,  -1.21207878e-02,  -3.16384770e-02,\n",
       "          3.82269733e-02,  -4.24392304e-03,  -5.96660959e-03,\n",
       "         -3.00485433e-03,   3.98821791e-03,   4.07789723e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(dot(A.T, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several big quantities (eg $-3.17336183e+14$), which seem disproportionate from the rest. Issue might be that we are assuming the  inverse does exist; but we better verify it rather than assuming.\n",
    "\n",
    "We know that $(A^T\\,A)^{-1}$ would exist if $A^T\\,A$ has full rank (that is, all columns or rows being linearly independent). We could know that by leveraging our Gauss-Jordan routine, but for brevity let us use a built-in routine to compute that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is (21L, 21L) and rank = 18\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "\n",
    "B = dot(A.T, A)\n",
    "print(\"Shape is %s and rank = %d\" % (B.shape, matrix_rank(B)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is telling us that there are 3 columns which are linearly dependent on the others; hence the matrix does not have full rank, and then, is not really invertible. \n",
    "\n",
    "Furthermore, even if it was invertible, Numerical Analysts do not recommend computing matrix $A^T\\,A$. This is because this matrix could be ill-conditioned respect to the problem of inverting it. We know that is not invertible, but let us just take a look at the condition number to reinforce this point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20695987672e+22\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import cond\n",
    "\n",
    "print(cond(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is huge! The order of this number, roughtly tells us how many digits of precision we will lose when computing the inverse, regardless of the round-off errors of the particular algorithm being used.  We should rather stay away from this approach then.\n",
    "\n",
    "Alright, so we screwed it by computing the inverse $(A^T\\,A)^{-1}$; but what can we do then for this case? Well, ultimately we want to approximately solve the over-determined linear system $A\\,w = b$. If the matrix $A^T\\,A$ is full-rank, then is invertible and we can use the well-known formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "    w = A^+\\,b = (A^T\\,A)^{-1}\\,A^T\\,b\n",
    "\\end{equation*}\n",
    "\n",
    "where the matrix $A^+$ is called the Moore–Penrose pseudo-inverse. But that is not the most general way of defining the pseudo-inverse matrix; we can still compute it when $A^T\\,A$ is not invertible, by using the SVD factorization. Let $U\\,\\Sigma\\,V^T = A$ be the SVD factorization of our data matrix $A$; then the pseudo-inverse can be computed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    A^+ = V\\,\\Sigma^+\\,U^T\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\Sigma^+$ is the pseudo-inverse of the diagonal matrix $\\Sigma$ (which contains the singular values of $A$). $\\Sigma^+$ can be easily computed  by taking the multiplicative inverse of the non zero singular values, and then transposing (this last step is not really needed, as diagonal matrices are symmetric; hence transposing does not change them). \n",
    "\n",
    "Using this more general definition of the pseudo-inverse, we can compute our model parameters $w$ as:\n",
    "\n",
    "\\begin{equation*}    \n",
    "    w = V\\,\\Sigma^+\\,U^T\\,b \\;\\ni\\; \n",
    "    U\\,\\Sigma\\,V^T = svd(A) \n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough talking, let us move to the code. Please note that we are setting to zero the quite small values of $\\Sigma$, as they are numerical zeros anyway (recall that $rank(A) = 18 < 21$). This is to avoid these small numbers to propagate further numerical errors in the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_prob=0.968434, error_prob=0.031566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "U,S,Vt = svd(A, full_matrices=0)\n",
    "S[S <= 1e-10] = 0\n",
    "Sp = np.diag(np.reciprocal(S, where=S>0))\n",
    "w = dot(dot(Vt.T, dot(Sp, U.T)), b)\n",
    "pred = np.sign(dot(A, w))\n",
    "n = float(b.shape[0])\n",
    "accuracy_prob = np.sum(pred == b) / n\n",
    "error_prob = 1 - accuracy_prob\n",
    "print(\"accuracy_prob=%f, error_prob=%f\\n\" % (accuracy_prob, error_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see then, that by doing proper numerical computations, we can get an accuracy probability of 96.8%, with respective and error probability of approximately %3. Not bad for a simple linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Given that $Ax = b - $(noise $e$ with mean zero). Get the error vector and multiply by $(A^T\\,A)^{-1}\\,A^T$.\n",
    "\n",
    "First of all, let us assume that these questions are on a different context than previous section; because here we need that  $(A^T\\,A)^{-1}$ does exist. We also call $\\hat{x}$ the approximate solution given by the least square errors method. Hence, \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{x} = (A^T\\,A)^{-1}\\,A^Tb\n",
    "\\end{equation*}\n",
    "\n",
    "###### i. What is the error?\n",
    "\n",
    "Let define the error $e$ as the difference between expected $b$ and our approximation $A\\hat{x}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "e = A\\hat{x} - b\n",
    "\\end{equation*}\n",
    "\n",
    "This question is asking us to project this error vector into the column space (by pre-multiplying by matrix $(A^T\\,A)^{-1}\\,A^T$), and see what happens. There is a little abuse of language here, as that matrix gives us the projection but in terms of the base given by columns of $A$; not in terms of the canonical base:\n",
    "\n",
    "\\begin{align*}\n",
    "(A^T\\,A)^{-1}\\,A^T\\,e &= (A^T\\,A)^{-1}\\,A^T\\ (A\\hat{x} - b) \\\\\n",
    "&= (A^T\\,A)^{-1}\\,(A^TA)\\,\\hat{x} - (A^T\\,A)^{-1}\\,A^Tb \\\\\n",
    "&= \\hat{x} - \\hat{x}  \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "We can see that the error $e$, when projected into $C(A)$, vanishes. This comes as no surprise, as the least square error method asks this error to be orthogonal to $C(A)$; in other words, orthogonality is the condition that brings the normal equations to life:\n",
    "\n",
    "\\begin{equation*}\n",
    "A^{T}(b - A\\hat{x}) = 0 \\;\\iff\\; A^TA\\hat{x} = A^Tb \\;\\iff\\; \\hat{x} = (A^TA)^{-1}A^Tb\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, if $e \\in C(A)^\\perp = N(A^T)$, then is expected than when we compute its dot product against all columns of $A$ (term $A^Tx$ from the formula), it vanishes to zero.\n",
    "\n",
    "###### ii. Define what is an unbiased error?\n",
    "\n",
    "I think that this question meant to ask about \"unbiased estimator\", given the context of least square error method, which relates to linear regression. In general, let us say we want to estimate the parameter $\\theta$ of an statistical model. Using concrete data from the population,  we run certain algorithm to compute $\\hat{\\theta}$; which is expected to be close to the real parameters vector $\\theta$. This $\\hat{\\theta}$ is an estimator our our true parameters $\\theta$, and we say is unbiased if:\n",
    "\n",
    "\\begin{equation*}\n",
    "E[\\hat{\\theta}] = \\theta\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, if the expected value of our estimator is the real parameter. In more intuitive terms, if we take different random samples form a population; our algorithm is likely to give raise to different estimators $\\hat{\\theta}$. But if we repeat this experiment long enough, we will notice that on average the estimator $\\hat{\\theta}$ starts to converge to the true value $\\theta$.\n",
    "\n",
    "For the particular problem of linear regression, Gauss and Markov proved (there is a theorem named after them), that the estimator given by least square error method is unbiased (assuming that certain conditions are met). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
